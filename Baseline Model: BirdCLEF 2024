{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":19596,"databundleVersionId":1292430,"sourceType":"competition"},{"sourceId":25954,"databundleVersionId":2091745,"sourceType":"competition"},{"sourceId":33246,"databundleVersionId":3221581,"sourceType":"competition"},{"sourceId":44224,"databundleVersionId":5188730,"sourceType":"competition"},{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"},{"sourceId":1487019,"sourceType":"datasetVersion","datasetId":726237},{"sourceId":1487116,"sourceType":"datasetVersion","datasetId":726312},{"sourceId":1664376,"sourceType":"datasetVersion","datasetId":985270},{"sourceId":5181249,"sourceType":"datasetVersion","datasetId":3012199},{"sourceId":5195317,"sourceType":"datasetVersion","datasetId":3020983},{"sourceId":6127,"sourceType":"modelInstanceVersion","modelInstanceId":4598}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Package imports","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # \"jax\" or \"tensorflow\" or \"torch\" \n\nimport keras_cv\nimport keras\nimport keras.backend as K\n\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\n# WandB for experiment tracking\n# Import wandb library for logging and tracking experiments\nimport wandb\nfrom wandb.keras import WandbCallback\n\n# Try to get the API key from Kaggle secrets\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"WANDB\")\n    # Login to wandb with the API key\n    wandb.login(key=api_key)\n    # Set anonymous mode to None\n    anonymous = None\nexcept:\n    # If Kaggle secrets are not available, set anonymous mode to 'must'\n    anonymous = 'must'\n    # Login to wandb anonymously and relogin if needed\n    wandb.login(anonymous=anonymous, relogin=True)\n    \n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport librosa\nimport tensorflow as tf\nimport tensorflow_io as tfio\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport IPython.display as ipd\nimport librosa.display as lid\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\ncmap = mpl.cm.get_cmap(\"coolwarm\")","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:24:14.826467Z","iopub.execute_input":"2024-04-30T03:24:14.827069Z","iopub.status.idle":"2024-04-30T03:24:53.636956Z","shell.execute_reply.started":"2024-04-30T03:24:14.827040Z","shell.execute_reply":"2024-04-30T03:24:53.635989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('np:', np.__version__)\nprint('pd:', pd.__version__)\nprint('sklearn:', sklearn.__version__)\nprint('librosa:', librosa.__version__)\nprint('tf:', tf.__version__)\nprint('keras:', keras.__version__)\nprint('kerasCV:', keras_cv.__version__)\nprint('tfio:', tfio.__version__)\nprint('w&b:', wandb.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:24:53.638862Z","iopub.execute_input":"2024-04-30T03:24:53.639464Z","iopub.status.idle":"2024-04-30T03:24:53.645931Z","shell.execute_reply.started":"2024-04-30T03:24:53.639438Z","shell.execute_reply":"2024-04-30T03:24:53.644978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"markdown","source":"## Config class:\n\nIncludes all kinds of configurations that are needed during the code. Is never instantiated, just a helper object. Also gets all the class names.","metadata":{}},{"cell_type":"code","source":"class CFG:\n    device = \"CPU\"\n    seed = 42\n    \n    # Input image size and batch size\n    img_size = [128, 384]\n    batch_size = 64\n    \n    # Audio duration, sample rate, and length\n    duration = 15 # second\n    sample_rate = 32000\n    audio_len = duration*sample_rate\n    \n    # STFT parameters\n    nfft = 2028\n    window = 2048\n    hop_length = audio_len // (img_size[1] - 1)\n    fmin = 20\n    fmax = 16000\n    \n    # Number of epochs, model name\n    epochs = 10\n    preset = 'efficientnetv2_b2_imagenet'\n    \n    # Data augmentation parameters\n    augment=True\n\n    # Class Labels for BirdCLEF 24\n    class_names = sorted(os.listdir('/kaggle/input/birdclef-2024/train_audio/'))\n    num_classes = len(class_names)\n    class_labels = list(range(num_classes))\n    label2name = dict(zip(class_labels, class_names))\n    name2label = {v:k for k,v in label2name.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:24:53.646891Z","iopub.execute_input":"2024-04-30T03:24:53.647194Z","iopub.status.idle":"2024-04-30T03:24:53.674388Z","shell.execute_reply.started":"2024-04-30T03:24:53.647172Z","shell.execute_reply":"2024-04-30T03:24:53.673710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG.device = 'CPU'","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:36.849231Z","iopub.execute_input":"2024-04-30T03:28:36.849898Z","iopub.status.idle":"2024-04-30T03:28:36.853768Z","shell.execute_reply.started":"2024-04-30T03:28:36.849870Z","shell.execute_reply":"2024-04-30T03:28:36.852876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Paths\nBelow you can find all the paths used for the competition (and other competitions, even though they *are not* used yet.). But later the idea is that we will use the other competitions as well, so it is handy to have the necessary paths already.\n\nBesides that, TPU of Kaggle needs GCS paths, so we check which device is stored in CFG. Right now we only train on GPU.","metadata":{}},{"cell_type":"code","source":"BASE_PATH0 = '/kaggle/input/birdsong-recognition'\nBASE_PATH1 = '/kaggle/input/birdclef-2021'\nBASE_PATH2 = '/kaggle/input/birdclef-2022'\nBASE_PATH3 = '/kaggle/input/birdclef-2023'\nBASE_PATH4 = '/kaggle/input/xeno-canto-bird-recordings-extended-a-m'\nBASE_PATH5 = '/kaggle/input/xeno-canto-bird-recordings-extended-n-z'\nBASE_PATH6 = '/kaggle/input/birdclef-2024'\n\n# This gets the Google Cloud Storage paths, if we are using TPU.\nif CFG.device==\"TPU\":\n    from kaggle_datasets import KaggleDatasets\n    GCS_PATH0 = KaggleDatasets().get_gcs_path(BASE_PATH0.split('/')[-1])\n    GCS_PATH1 = KaggleDatasets().get_gcs_path(BASE_PATH1.split('/')[-1])\n    GCS_PATH2 = KaggleDatasets().get_gcs_path(BASE_PATH2.split('/')[-1])\n    GCS_PATH3 = KaggleDatasets().get_gcs_path(BASE_PATH3.split('/')[-1])\n    GCS_PATH4 = KaggleDatasets().get_gcs_path(BASE_PATH4.split('/')[-1])\n    GCS_PATH5 = KaggleDatasets().get_gcs_path(BASE_PATH5.split('/')[-1])\n    GCS_PATH6 = KaggleDatasets().get_gcs_path(BASE_PATH6.split('/')[-1])\nelse:\n    GCS_PATH0 = BASE_PATH0\n    GCS_PATH1 = BASE_PATH1\n    GCS_PATH2 = BASE_PATH2\n    GCS_PATH3 = BASE_PATH3\n    GCS_PATH4 = BASE_PATH4\n    GCS_PATH5 = BASE_PATH5\n    GCS_PATH6 = BASE_PATH6","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:40.378532Z","iopub.execute_input":"2024-04-30T03:28:40.378902Z","iopub.status.idle":"2024-04-30T03:28:40.387254Z","shell.execute_reply.started":"2024-04-30T03:28:40.378873Z","shell.execute_reply":"2024-04-30T03:28:40.386231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data\nWe load the train metadata, because it contains useful information for naming each individual audio file.\nWe create extra columns for the filepath, the label that should be guessed, filename and the id of the recording on Xeno Canto.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f\"{BASE_PATH6}/train_metadata.csv\")\ndf['filepath'] = BASE_PATH6 + '/train_audio/' + df.filename\ndf['target'] = df.primary_label.map(CFG.name2label)\ndf['filename'] = df.filepath.map(lambda x: x.split('/')[-1])\ndf['xc_id'] = df.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:42.225642Z","iopub.execute_input":"2024-04-30T03:28:42.226061Z","iopub.status.idle":"2024-04-30T03:28:42.473530Z","shell.execute_reply.started":"2024-04-30T03:28:42.226029Z","shell.execute_reply":"2024-04-30T03:28:42.472607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading audio\nLibrosa is an audio based python library.\nIt loads the audio, and the sample rate of the audio.\n\nThen we convert into a mel spectrogram (see Weights and Biases for more details on Mel Spectrograms).\nAfterwards we do min max scaling. \n\nDefine function to display audio with audio player and spectrogram display.","metadata":{}},{"cell_type":"code","source":"def load_audio(filepath):\n    audio, sr = librosa.load(filepath)\n    return audio, sr\n\ndef get_spectrogram(audio):\n    spec = librosa.feature.melspectrogram(y=audio, \n                                   sr=CFG.sample_rate, \n                                   n_mels=256,\n                                   n_fft=2048,\n                                   hop_length=512,\n                                   fmax=CFG.fmax,\n                                   fmin=CFG.fmin,\n                                   )\n    spec = librosa.power_to_db(spec, ref=1.0)\n    min_ = spec.min()\n    max_ = spec.max()\n    if max_ != min_:\n        spec = (spec - min_)/(max_ - min_)\n    return spec\n\ndef display_audio(row):\n    # Caption for viz\n    caption = f'Id: {row.filename} | Name: {row.common_name} | Sci.Name: {row.scientific_name} | Rating: {row.rating}'\n    # Read audio file\n    audio, sr = load_audio(row.filepath)\n    # Keep fixed length audio\n    audio = audio[:CFG.audio_len]\n    # Spectrogram from audio\n    spec = get_spectrogram(audio)\n    # Display audio\n    print(\"# Audio:\")\n    display(ipd.Audio(audio, rate=CFG.sample_rate))\n    print('# Visualization:')\n    fig, ax = plt.subplots(2, 1, figsize=(12, 2*3), sharex=True, tight_layout=True)\n    fig.suptitle(caption)\n    # Waveplot\n    lid.waveshow(audio,\n                 sr=CFG.sample_rate,\n                 ax=ax[0],\n                 color= cmap(0.1))\n    # Specplot\n    lid.specshow(spec, \n                 sr = CFG.sample_rate, \n                 hop_length=512,\n                 n_fft=2048,\n                 fmin=CFG.fmin,\n                 fmax=CFG.fmax,\n                 x_axis = 'time', \n                 y_axis = 'mel',\n                 cmap = 'coolwarm',\n                 ax=ax[1])\n    ax[0].set_xlabel('');\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:44.329844Z","iopub.execute_input":"2024-04-30T03:28:44.330222Z","iopub.status.idle":"2024-04-30T03:28:44.341498Z","shell.execute_reply.started":"2024-04-30T03:28:44.330187Z","shell.execute_reply":"2024-04-30T03:28:44.340520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data splitting\nSplit the dataframe (the metadata dataframe!) into train and test.\nWe could later use CV, so we have more training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, valid_df = train_test_split(df, test_size = 0.2)\nprint(f\"Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:46.568559Z","iopub.execute_input":"2024-04-30T03:28:46.569125Z","iopub.status.idle":"2024-04-30T03:28:46.726973Z","shell.execute_reply.started":"2024-04-30T03:28:46.569094Z","shell.execute_reply":"2024-04-30T03:28:46.726092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_loader():\n    def get_audio(path):\n        # Function that gets the audio \n        file_bytes = tf.io.read_file(path)\n        audio = tfio.audio.decode_vorbis(file_bytes)\n        audio = tf.cast(audio, tf.float32)\n        if tf.shape(audio)[1] > 1:\n            audio = audio[...,0:1]\n        audio = tf.squeeze(audio, axis=-1)\n        return audio","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:47.985296Z","iopub.execute_input":"2024-04-30T03:28:47.985633Z","iopub.status.idle":"2024-04-30T03:28:47.991728Z","shell.execute_reply.started":"2024-04-30T03:28:47.985607Z","shell.execute_reply":"2024-04-30T03:28:47.990601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the decoder\nThis utilizes the builder pattern to build a number of functions and returns that decoding function itself.\nBy calling build_decoder, we define the functions in it, and the respective decoding function, and return it.\n\nFirst we get the audio using a path. They are stored in an ogg. file format, so we reading the file using tf-io, which is a Tensorflow library for input/output handling.\nWe decode using a decoding method in decode_vorbis.\nWe cast the audio data to float32.\nIf the second dimension of the audio is bigger than one, we only take the first dimension. \"...\" is the Ellipsis operator in Python and basically says, \"every dimension in between\".\n\nThen we crop the audio. Pad_mode is a specific tensorflow padding parameter. \nWe decide based on whether the audio file is too long or too short whether to crop or pad it. \nPadding pads a tensor according to the paddings you specify. paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension. If mode is \"REFLECT\" then both paddings[D, 0] and paddings[D, 1] must be no greater than tensor.dim_size(D) - 1. If mode is \"SYMMETRIC\" then both paddings[D, 0] and paddings[D, 1] must be no greater than tensor.dim_size(D).\n\nIf we crop, we basically select a random int from 0 to the difference in length and take a specific portion from the audio.\n\napply_preproc applies Z score scaling and min max scaling to the spec. \n\nGet label is a function to get the target label of the dataframe. \n\nDecode calls each of these functions and reshapes the audio into a 3 channel spec.","metadata":{}},{"cell_type":"code","source":"def build_decoder(with_labels = True, dim=1024):\n    def get_audio(path):\n        # Function that gets the audio \n        file_bytes = tf.io.read_file(path)\n        audio = tfio.audio.decode_vorbis(file_bytes)\n        audio = tf.cast(audio, tf.float32)\n        if tf.shape(audio)[1] > 1:\n            audio = audio[...,0:1]\n        audio = tf.squeeze(audio, axis=-1)\n        return audio\n    def crop_or_pad(audio, target_len, pad_mode=\"constant\"):\n        audio_len = tf.shape(audio)[0]\n        diff_len = abs(target_len - audio_len)\n        if target_len > audio_len:\n            pad1 = tf.random.uniform([], maxval = diff_len, dtype= tf.int32)\n            pad2 = diff_len-pad1\n            audio = tf.pad(audio, paddings=[[pad1, pad2]], mode = pad_mode)\n        elif audio_len > target_len:\n            idx = tf.random.uniform([], maxval = diff_len, dtype=tf.int32)\n            audio = audio[idx : (idx + target_len)]\n        return audio\n    def apply_preproc(spec):\n        mean = tf.math.reduce_mean(spec)\n        std = tf.math.reduce_std(spec)\n        spec = tf.where(tf.math.equal(std, 0), spec - mean, (spec - mean)/std)\n\n        min_val = tf.math.reduce_min(spec)\n        max_val = tf.math.reduce_max(spec)\n        spec = tf.where(tf.math.equal(max_val - min_val, 0), spec - min_val, (spec - min_val)/(max_val-min_val))\n        return spec\n    def get_label(label):\n        label = tf.reshape(label, [1])\n        label = tf.cast(tf.one_hot(label, CFG.num_classes), tf.float32)\n        label = tf.reshape(label, [CFG.num_classes])\n        return label\n    def decode(path):\n        audio = get_audio(path)\n        audio = crop_or_pad(audio, CFG.audio_len)\n        spec = keras.layers.MelSpectrogram(num_mel_bins = CFG.img_size[0],\n                                          fft_length = CFG.nfft,\n                                          sequence_stride = CFG.hop_length,\n                                          sampling_rate = CFG.sample_rate)(audio)\n        spec = apply_preproc(spec)\n        spec = tf.tile(spec[..., None], [1,1,3])\n        spec = tf.reshape(spec, [*CFG.img_size, 3])\n        return spec\n    def decode_with_labels(path, label):\n        label = get_label(label)\n        return decode(path), label\n    return decode_with_labels if with_labels else decode","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:28:49.234276Z","iopub.execute_input":"2024-04-30T03:28:49.234633Z","iopub.status.idle":"2024-04-30T03:28:49.249615Z","shell.execute_reply.started":"2024-04-30T03:28:49.234606Z","shell.execute_reply":"2024-04-30T03:28:49.248670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data augmentation\nThis mixes two images linearly, randomly cuts out a portion of time and randomly cuts out a frequency.\nThen applies each augmenter to the data.","metadata":{}},{"cell_type":"code","source":"def build_augmenter():\n    # This seems to only have image augmentation. To add gaussian noise might need audio..\n    augmenters = [\n        keras_cv.layers.MixUp(alpha=0.4),\n        keras_cv.layers.RandomCutout(height_factor=(1.0, 1.0), width_factor = (0.06, 0.12)), # time-masking\n        keras_cv.layers.RandomCutout(height_factor=(0.06, 0.1), width_factor= (1.0, 1.0)) # freq-masking\n    ]\n    def augment(img, label):\n        data = {'images': img, 'labels': label}\n        for augmenter in augmenters:\n            if tf.random.uniform([]) < 0.35:\n                data = augmenter(data, training=True)\n        return data['images'], data['labels']\n    return augment","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:29:06.649817Z","iopub.execute_input":"2024-04-30T03:29:06.650518Z","iopub.status.idle":"2024-04-30T03:29:06.657073Z","shell.execute_reply.started":"2024-04-30T03:29:06.650489Z","shell.execute_reply":"2024-04-30T03:29:06.656172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build dataset\nThis builds the dataset. First we get the decoding and augmentation functions.\n\nThis is agnostic to whether we pass the targets or not (as it should because we don't have them in the test case).\n\nIt performs some operations to bring the data in a format so that Tensorflow can build a dataset out of it. This utilizes the tensorflow Data API. Pretty cool, and pretty efficient. It also uses the decoding function.\nIt also shuffles the data, batches the data, and applies the augmenting. This is where we would add gaussian noise addition.\n\nThe cacheing operation makes the data loading more efficient. That is the reason why the first epoch is very slow, but later epochs are very fast, because the data is cached.","metadata":{}},{"cell_type":"code","source":"def build_dataset(paths, labels=None, batch_size=32,\n                  loader_fn=None,decode_fn=None, augment_fn=None, \n                  cache=True,augment=False, shuffle=2048):\n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None, dim = CFG.audio_len)\n    if augment_fn is None:\n        augment_fn = build_augmenter()\n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = (paths, ) if labels is None else (paths, labels)\n    # dataset created here (believe each item is a path:label match)\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    ds = ds.cache() if cache else ds\n    \n    if shuffle:\n        opt = tf.data.Options()\n        ds = ds.shuffle(shuffle, seed = CFG.seed)\n        opt.deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.batch(batch_size = batch_size, drop_remainder = True)\n    ds = ds.map(augment_fn, num_parallel_calls = AUTO) if augment else ds\n    ds = ds.prefetch(AUTO)\n    return ds\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:43:29.373885Z","iopub.execute_input":"2024-04-30T03:43:29.374599Z","iopub.status.idle":"2024-04-30T03:43:29.384571Z","shell.execute_reply.started":"2024-04-30T03:43:29.374569Z","shell.execute_reply":"2024-04-30T03:43:29.383596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a testing section to show what I have in my dataset. \n# as I wanted to change the build process\nds_paths = valid_df.filepath.values\nds_labels = valid_df.target.values\nds = build_dataset(ds_paths, ds_labels, batch_size = 1, \n                   shuffle = False, augment = True,\n                  pfetch = False, parallel = False)\nds_iter = ds.as_numpy_iterator()\nds_example = next(ds_iter)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:59:33.106454Z","iopub.execute_input":"2024-04-30T03:59:33.107153Z","iopub.status.idle":"2024-04-30T03:59:34.449735Z","shell.execute_reply.started":"2024-04-30T03:59:33.107123Z","shell.execute_reply":"2024-04-30T03:59:34.448745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can preview the data in dataset.\n# I will at least know after I change the code that things\n# don't break in completeness.\nprint(ds_example[1][0])\n#plt.imshow(ds_example[0][0])\nlid.specshow(ds_example[0][0], \n                 sr = CFG.sample_rate, \n                 hop_length=512,\n                 n_fft=2048,\n                 fmin=CFG.fmin,\n                 fmax=CFG.fmax,\n                 x_axis = 'time', \n                 y_axis = 'mel',\n                 cmap = 'coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:04:50.242350Z","iopub.execute_input":"2024-04-30T04:04:50.242715Z","iopub.status.idle":"2024-04-30T04:04:50.577591Z","shell.execute_reply.started":"2024-04-30T04:04:50.242688Z","shell.execute_reply":"2024-04-30T04:04:50.576701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_paths = train_df.filepath.values\ntrain_labels = train_df.target.values\ntrain_ds = build_dataset(train_paths, train_labels, batch_size = CFG.batch_size, shuffle = True, augment= CFG.augment)\n\nvalid_paths = valid_df.filepath.values\nvalid_labels = valid_df.target.values\nvalid_ds = build_dataset(valid_paths, valid_labels, batch_size = CFG.batch_size, shuffle = False, augment = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:08.653136Z","iopub.execute_input":"2024-04-28T11:56:08.653744Z","iopub.status.idle":"2024-04-28T11:56:10.384743Z","shell.execute_reply.started":"2024-04-28T11:56:08.653714Z","shell.execute_reply":"2024-04-28T11:56:10.383691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_batch(batch, row = 3, col = 2, label2name = None):\n    if isinstance(batch, tuple) or isinstance(batch, list):\n        specs, tars = batch\n    else:\n        specs = batch\n        tars = None\n        \n    plt.figure(figsize=(col*5, row*3))\n    \n    for idx in range(row * col):\n        ax = plt.subplot(row, col, idx + 1)\n        lid.specshow(np.array(specs[idx, ..., 0]),\n                    n_fft = CFG.nfft, \n                    hop_length = CFG.hop_length,\n                    sr = CFG.sample_rate,\n                    x_axis = 'time',\n                    y_axis = 'mel',\n                    cmap = 'coolwarm')\n        if tars is not None:\n            label = tars[idx].numpy().argmax()\n            name = label2name[label]\n            plt.title(name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:10.385930Z","iopub.execute_input":"2024-04-28T11:56:10.386436Z","iopub.status.idle":"2024-04-28T11:56:10.396293Z","shell.execute_reply.started":"2024-04-28T11:56:10.386405Z","shell.execute_reply":"2024-04-28T11:56:10.395539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_ds = train_ds.take(100)\nbatch = next(iter(sample_ds))\nplot_batch(batch, label2name = CFG.label2name)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:10.399550Z","iopub.execute_input":"2024-04-28T11:56:10.408158Z","iopub.status.idle":"2024-04-28T11:56:30.144977Z","shell.execute_reply.started":"2024-04-28T11:56:10.408127Z","shell.execute_reply":"2024-04-28T11:56:30.144080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model compilation\nNow we build a model using an agnostic input shape, the backbone, which is EfficientNet, and then build a classifier with it.\n\nOptimizer is adapt, and the loss is CrossEntropy.","metadata":{}},{"cell_type":"code","source":"inp = keras.layers.Input(shape=(None, None, 3))\nbackbone = keras_cv.models.EfficientNetV2Backbone.from_preset(CFG.preset)\nout = keras_cv.models.ImageClassifier(\n    backbone= backbone,\n    num_classes = CFG.num_classes,\n    name = 'classifier')(inp)\nmodel = keras.models.Model(inputs = inp, outputs = out)\nmodel.compile(optimizer = \"adam\",\n             loss = keras.losses.CategoricalCrossentropy(label_smoothing=0.02),\n             metrics = [keras.metrics.AUC(name='auc')])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:30.146681Z","iopub.execute_input":"2024-04-28T11:56:30.147364Z","iopub.status.idle":"2024-04-28T11:56:36.889153Z","shell.execute_reply.started":"2024-04-28T11:56:30.147327Z","shell.execute_reply":"2024-04-28T11:56:36.888155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning rate scheduler\n\nWe need a flexible learning rate. This function defines a learning rate callback for Keras. First, we linearly increase during a period of lr_ramp_ep. Then we sustain this for a period of lr_sus_ep. Then we either exponentially, step-wise or cosine wise relax this learning rate.","metadata":{}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size = 8, mode='cos', epochs = 10, plot = False):\n    lr_start, lr_max, lr_min = 5e-5, 8e-6 * batch_size, 1e-5\n    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_min)/lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max-lr_min) * lr_decay**(epoch-lr_ramp_ep-lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos': \n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min)*0.5*(1+math.cos(phase)) + lr_min\n        return lr\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:36.890423Z","iopub.execute_input":"2024-04-28T11:56:36.890765Z","iopub.status.idle":"2024-04-28T11:56:36.909874Z","shell.execute_reply.started":"2024-04-28T11:56:36.890733Z","shell.execute_reply":"2024-04-28T11:56:36.909028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, mode='cos', plot=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:36.911833Z","iopub.execute_input":"2024-04-28T11:56:36.913230Z","iopub.status.idle":"2024-04-28T11:56:37.277345Z","shell.execute_reply.started":"2024-04-28T11:56:36.913192Z","shell.execute_reply":"2024-04-28T11:56:37.276412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(\"best_model.weights.h5\",\n                                         monitor = 'val_auc',\n                                         save_best_only = True,\n                                         save_weights_only = True,\n                                         mode='max')","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:37.281493Z","iopub.execute_input":"2024-04-28T11:56:37.283568Z","iopub.status.idle":"2024-04-28T11:56:37.291977Z","shell.execute_reply.started":"2024-04-28T11:56:37.283534Z","shell.execute_reply":"2024-04-28T11:56:37.291089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\n\nclass LRSchedulerLogger(Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        # Log the learning rate at the beginning of the epoch\n        if callable(lr):  # Check if the learning rate is a callable (common in TF 2.x)\n            lr = lr(self.model.optimizer.iterations)\n        if hasattr(lr, 'numpy'):\n            lr = lr.numpy()  # For TF 2.x compatibility\n        wandb.log({'learning_rate': lr}, commit=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:17:00.311187Z","iopub.execute_input":"2024-04-28T11:17:00.311450Z","iopub.status.idle":"2024-04-28T11:17:00.322090Z","shell.execute_reply.started":"2024-04-28T11:17:00.311427Z","shell.execute_reply":"2024-04-28T11:17:00.321318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"Birdclef-2024-public\", config={\n    \"img_size\": CFG.img_size,\n    \"batch_size\": CFG.batch_size,\n    \"duration\": CFG.duration,\n    \"sample_rate\": CFG.sample_rate,\n    \"audio_len\": CFG.audio_len,\n    \"nfft\": CFG.nfft,\n    \"window\": CFG.window,\n    \"hop_length\": CFG.hop_length,\n    \"fmin\": CFG.fmin,\n    \"fmax\": CFG.fmax,\n    \"epochs\": CFG.epochs,\n    \"preset\": CFG.preset,\n    \"augment\": CFG.augment,\n    \"num_classes\": CFG.num_classes\n})\n\n #  WandbCallback(), \n  #  LRSchedulerLogger()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the GPU as the default device\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.set_visible_devices(physical_devices[0], 'GPU')\nlogical_devices = tf.config.list_logical_devices('GPU')\nprint(len(physical_devices), \"Physical GPUs,\", len(logical_devices), \"Logical GPUs\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:50:10.902276Z","iopub.execute_input":"2024-04-28T11:50:10.902988Z","iopub.status.idle":"2024-04-28T11:50:10.911048Z","shell.execute_reply.started":"2024-04-28T11:50:10.902943Z","shell.execute_reply":"2024-04-28T11:50:10.909962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Callbacks and model fitting\nNow, we defined some callbacks which are useful for keeping track of training.\nThen we simply select the GPU (usually automatically selected, but just to be explicit/sure).\nThen we fit the model. The model check points saves the best model.","metadata":{}},{"cell_type":"code","source":"# Initialize a new run\n\ncallbacks = [\n    lr_cb, \n    ckpt_cb,\n]\nwith tf.device(\"/GPU:0\"):\n    history = model.fit(train_ds,\n                       validation_data = valid_ds,\n                       epochs = CFG.epochs,\n                       callbacks = callbacks,\n                       verbose = 1\n                       )\n\n#wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:56:37.309535Z","iopub.execute_input":"2024-04-28T11:56:37.309865Z","iopub.status.idle":"2024-04-28T13:03:16.514923Z","shell.execute_reply.started":"2024-04-28T11:56:37.309837Z","shell.execute_reply":"2024-04-28T13:03:16.513937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmax(history.history[\"val_auc\"])\nbest_score = history.history[\"val_auc\"][best_epoch]\nprint('>>> Best AUC: ', best_score)\nprint('>>> Best Epoch: ', best_epoch+1)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:14:17.844969Z","iopub.execute_input":"2024-04-28T13:14:17.845348Z","iopub.status.idle":"2024-04-28T13:14:17.854565Z","shell.execute_reply.started":"2024-04-28T13:14:17.845320Z","shell.execute_reply":"2024-04-28T13:14:17.853185Z"},"trusted":true},"execution_count":null,"outputs":[]}]}